<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Net_-_Data_Lakes_-_Data_Bricks_-_CA_-_Mehdi</title>
    <style>
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }

        .resume-container {
            max-width: 850px;
            margin: 0 auto;
            background-color: white;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            padding: 40px;
            position: relative;
        }

        @media screen and (max-width: 768px) {
            body { padding: 10px; }
            .resume-container { padding: 20px; }
        }

        @media print {
            body { background-color: white; padding: 0; }
            .resume-container { box-shadow: none; max-width: 100%; padding: 0; }
        }

        /* Typography */
        h1 { font-size: 2.5em; margin-bottom: 0.5em; color: #2c3e50; }
        h2 { font-size: 1.8em; margin-top: 1em; margin-bottom: 0.5em; color: #34495e; border-bottom: 2px solid #3498db; padding-bottom: 0.3em; }
        h3 { font-size: 1.4em; margin-top: 0.8em; margin-bottom: 0.4em; color: #34495e; }
        h4 { font-size: 1.2em; margin-top: 0.6em; margin-bottom: 0.3em; color: #34495e; }
        h5, h6 { font-size: 1.1em; margin-top: 0.5em; margin-bottom: 0.3em; color: #34495e; }

        p { margin-bottom: 0.8em; text-align: justify; }

        /* Lists */
        ul, ol { margin-left: 20px; margin-bottom: 0.8em; }
        li { margin-bottom: 0.3em; }

        /* Tables */
        table { width: 100%; border-collapse: collapse; margin-bottom: 1em; }
        th, td { padding: 8px 12px; text-align: left; border-bottom: 1px solid #ddd; }
        th { background-color: #f8f9fa; font-weight: bold; }

        /* Images */
        img { max-width: 100%; height: auto; display: block; margin: 1em auto; }
        .inline-image { display: inline-block; vertical-align: middle; margin: 0 5px; }

        /* Text formatting */
        strong, b { font-weight: 600; color: #2c3e50; }
        em, i { font-style: italic; }
        u { text-decoration: underline; }

        /* Links */
        a { color: #3498db; text-decoration: none; }
        a:hover { text-decoration: underline; }

        /* Special sections */
        .header-section { text-align: center; margin-bottom: 2em; }
        .contact-info { margin-bottom: 1em; }
        .section { margin-bottom: 2em; }

        /* Preserve original spacing */
        .preserve-space { white-space: pre-wrap; }

        /* Page breaks for printing */
        .page-break { page-break-after: always; }

        /* Highlighted text */
        .highlight { background-color: #fffacd; padding: 2px 4px; }

        /* Custom fonts embedded */
        @font-face {
            font-family: 'ResumeFont';
            src: local('Arial'), local('Helvetica');
        }
        
    </style>
</head>
<body>
    <div class="resume-container">
        <p><strong>Mehdi Haghdad</strong></p><p>62 Parkcrest</p><p>Newport Beach, CA 92612</p><p>Cell phone: (949) 393-1150</p><p><a href="mailto:mhaghdad2014@gmail.com">mhaghdad2014@gmail.com</a></p><p>EDUCATION</p><p>PhD, UCLA/University of California Davis	Obtained on 6/2003</p><p>PhD in Electrical and computer engineering (in Smart Antenna Systems for Low Earth Orbit (LEO) Satellites)</p><p>The Royal Institute Of Technology, Stockholm, Sweden</p><p>M.S. Degree: Telecommunications Engineering.</p><p>B.S. Degree: Electrical Engineering </p><p>B.S. Degree: Computer Science.</p><p>SUMMARY</p><p>I have almost 20 years of experience in advance engineering, 10 years of experience in big data, 8 years in cloud, Multi-Cloud, Azure, AWS, GCP, 7 years in Artificial Intelligence, ML DL CNN. <strong>I have worked for some of the most prestigious companies in Silicon Valley and other locations such as Lockheed Martin, Space Systems Loral, Microsoft (4 times as a senior solution architect for different multi-billion dollar clients), Hewlett Packard, Texas Instruments, Optum, Acxiom, Citibank, United Healthcare, Ericsson, ABB Atom, Adaptec Broad Logic, Dell, Argonaut Technologies, Bausch &amp; Lomb, DARPA, Department of Defense (DOD) etc.</strong></p><p>I have been responsible for the development of many systems and applications from the designing board to the commercial release.</p><p>OBJECTIVE</p><p>Primarily looking for consulting and contract work but also open to permanent positions. </p><p>Senior solution architect Hands On!!, Senior developer, Architect, DevOps for advance engineering in Mulit-Cloud (Azure, AWS and GCP) Big Data, Artificial Intelligence. Can also help to build team and lead projects from conceptual design to commercial release.</p><p>Senior Big Data and Cloud Architect Consultant / BI Solutions Architect / Data Management and Cloud Lead/ DevOps</p><p>SKILLS</p><ul><li><strong>18 years</strong> of solid working experience with a PhD from University of California UCLA/Davis </li><li><strong>17 years of experience</strong> as <strong>Hands-On</strong> Solution Architect, Team Lead, Specialist, Developer, doing Senior Big Data and Cloud Architect Consultant / BI Solutions Architect / Data Management and Cloud Lead</li><li>Cloud Architecture, Big Data Engineering, Machine Learning, Deep Learning, Data Scientist, Business Intelligence (BI), Data Warehousing System Engineering multi-tiered applications.</li><li><strong>9+ years of experience in Big Data</strong>, Hadoop, Spark, MapR, Cloudera, Hortonworks, Storm, Kafka Confluent, Hive, Impala, Flume, Sqoop, MapReduce, Pig, HBase, NiFi, oozie, Tableau Power BI, Cloudera visualization, QlikView Scala SBT (My Preference), Java Maven etc.</li><li>Expert in <strong>Big Data: </strong>HDFS, Hive, Spark, Scala, Java, Python, Hadoop, Sqoop, Map/Reduce, Hortonworks, Cloudera, MapR, NoSQL HBase, Cassandra, Kafka Confluent, Storm, Spark Streaming, Zeppelin, Kibana, Spark MLib, Mahout, R, NiFi, Falcon, Oozie, Kylin, Atlas, Drill, Solr, ElasticSearch, Ambari,Ranger, Flume, Impala, Pig, HDInsight, SBT etc.</li><li><strong>8 years of Azure cloud and Multi-Cloud </strong>Hands on!! Extensive full cycle Azure Cloud experience with full Big Data, Machine Learning Deep Learning, Azure Machine Learning Studio, Azure Power BI, and Azure Search. Comprehensive architecture, development and deployments of massive Azure infrastructures for prestigious companies including 4 times for Microsoft as senior solution architect. Full cycle Data Lake Design, Data Engineering and Pipelines, Data Streaming, Data Fabric, Big Data and Artificial Intelligence design and implementation on massive scale. Used Azure Data Factory (ADF Gen1 and Gen2), Azure Datalake Storage (ADLS Gen1 and Gen2), SQL/NoSQL on Cloud, HDInsight, Databricks, Databricks Delta Lake, Infrastructure as Code, Cloud Governance, Azure CLI, Azure PowerShell, Python, DevOps, Kubernetes, Docker, AKS Azure Kubernetes, CI/CD(CICD, CI CD,CI-CD) tools, MLOps, Machine Learning, Deep Learning, Azure Machine Learning Studio, Azure Search, and Elasticsearch, Azure DevOps with automated build and release CI/CD(CICD, CI CD,CI-CD) pipelines utilizing ARM templates, Terraform, Jenkins, Ansible, Azure CLI, Automations, Runbooks, Build Tools, Azure Data Lake Store, Azure Data Lake Analytics, HDInsight (Managed Hadoop), Databricks, Stream Analytics, Machine Learning Studio, Azure DatabricksÂ¸ Azure Data Factory, Azure Data Flow, Azure Data Warehouse, Azure Synapse, Azure SQL Data Warehouse (Azure SQL DW), Azure Analysis Services, Event Hubs, Power BI, and Azure Data Catalog, Atlas, Collibra Data Intelligence, Erwin Data Governance. Azure security Azure Active Directory AAD, AD. AAD-DS. ESP, MFA, IR Integration Runtime Gateway, Domain join, Apache Ranger, Ambari, Advanced 4 pillar of security for HDInsight (perimeter VNET, Kerberos AD ESP authentication, Hive policies Ranger and data encryption) Azure Spark Hive HBase Hadoop Kafka Confluent Storm ML Services (R Server) HDInsight clusters, Domain-joined HDInsight clusters, Azure Zeppelin notebooks, Azure Jupyter notebooks, Azure SQL Data Warehouses, Azure Databricks, Azure Data Lake, Azure Data Lake Factory, Azure Data Lake Storage, Azure Data Lake Analytics, Azure Data Links, Azure Integration Runtime (IR), Azure Data Gateway, Azure Kubernetes Services, Azure Storage Blobs, Azure Active Directory, Azure Service Principals, Azure Security Center, Azure Key Vaults, Azure Virtual Network vnet, Azure Log Analytics, Azure Network Interfaces, Azure Cosmos DB, Azure Cortana Intelligence Suite, Infrastructure as a Service IaaS, Platform as a Service PaaS, Microsoft R Server, NLB, Key phrase extraction Azure search, Unstructured text analytics, Event hub, Streaming, Poly Base etc.</li><li><strong>8 years of AWS Cloud</strong> Extensive full cycle Cloud AWS Redshift, RDS, EMR, Kinesis, S3, Glue, DMS, Athena, EC2, Lambda, experience with full Big Data, Amazon Elastic MapReduce (EMR), Hadoop, Spark, Hive, Pig, Kafka Confluent, MSK, AWS Management Console, AWS CLI, Amazon EMR File System (EMRFS); Comprehensive CDK and CloudFormation experience with writing script in TypeScript, JavaScript, Python, Java, and C#.  Collaborative notebooks Apache Zeppelin, Jupyter, deep learning frameworks like Apache MXNet, RDS Aurora MySQL PostgreSQL, Elasticsearch and SOLR, Machine Learning and Deep Learning development and deployment. AWS Compute E2C, FarGate, Lambda, VMware, AWS Developer Tools, AWS Management Tools, Amazon Machine Learning, AWS DeepLens, Amazon Deep Learning AIMs, Amazon TensorFlow on AWS and other components etc.                                                                .</li><li><strong>5 years of Google Cloud (GCP)</strong> Architected and implemented multiple massive projects for different companies Hands On!! Including Big Data, Artificial Intelligence, SOLR on Kubernetes indexing from Dataproc directly. Used among others Cloud Dataproc, BigQuery, Cloud Dataflow, Cloud Data Fusion, Cloud Dataprep, Data Catalog, Google Kubernetes Engine (GKE), Kubernetes CLusters, Dataproc CLusters, Container Registry, Deep Learning Containers, Cloud Bigtable, Cloud SQL, Firebase Realtime Database. For artificial intelligence used AI building blocks, Text-to-Speech, Speech-to-Text, AutoML, Vision AI, Cloud Natural Language, Video AI, AI Platform, AI Hub and AI Platform Deep Learning VM Image etc.</li><li><strong>7 years of Machine Learning, Deep Learning and Artificial Intelligence </strong>MLlib, TensorFlow, Keras, Weka Mahout, Multilayer perceptron classifier (MLPC), the feedforward artificial neural network, Convolutional Neural Network CNN, scikit-learn, Pandas, Deeplearning4j, H2o, Sparkling Water ML, Caffe2, MxNet etc. Different algorithms K-Means, Random Forest, Gradient Boosting algorithms (GBM, XGBoost and CatBoost) etc.</li><li><strong>9 years of search engines </strong>ELK Stack<strong> </strong>Elasticsearch, Logstash, Kibana, Filebeat, SOLR from early versions until 8.4.2, SOLR on Kubernetes on GCP!!, Lucene, Rsync, Tika. Also been involved with migration from SOLR to Elasticsearch for at least three companies etc.</li><li>Expert in <strong>DevOps</strong> using Azure DevOps, VSTS, AWS-CodePipeline, Terraforms, Jenkins, Ansible, Git, Maven, Cloudera Navigator, Data Lineage, Kubernetes, Docker. </li><li><strong>5 years of Solid Kubernetes, Orchestration and Micro Services experience</strong> Distributed Container based Architecture, Docker, Docker CLI, Kubernetes, Kubernetes CLI, kubeflow, Kube-scheduler, Pods, Pods deployments. Service Deployments, Ingress, Helm Charts, Helm Charts CLI, YAML etc. Successfully installed and deployed for several companies huge Kubernetes Cluster (the latest with more than 2000 nodes 16,000 CPUs deployed in 20 minutes) with both Data Pipeline, Data Lake (Raw Data Landing Zone, Processing Zone, Consumption Zone) for Machine Learning, Convolutional Neural Network CNN, Spark on Kubernetes, HDFS on Kubernetes, Kafka Confluent on Kubernetes, Elasticsearch Logstash Kibana ELK on Kubernetes, NiFi on Kubernetes, Hive HBase Jupyter Zeppelin with Scala and Python on Kubernetes. Used Vagrant Terraform HashiCorp for deployment with 2000 nodes, 16,000 CPUs with Peta Bytes and 150TB / day capacity etc.</li><li><strong>3 years of IBM Cloud and IBM Cloud Private (ICP)</strong> Distributed Container based Architecture, Docker, Docker CLI, Kubernetes, Kubernetes CLI, Pods, Pods deployments. Service Deployments, Ingress, Helm Charts, Helm Charts CLI. Successfully installed and deployed an entire IBM Cloud Private ICP Cluster then implemented and deployed ELK Elasticsearch, Logstash, Kibana, Filebeat, Kafka Confluent, Zookeeper, Cassandra, Curator on ICP IBM Private Cloud, Kubernetes, Pods using Helm Charts, Scala SBT.</li><li><strong>16 years of hands on .NET development</strong>, architecture and management experience in application, real time, instrumentation, web, front end, back end, full stack, multiple products out there multiple awards</li><li><strong>16 years of hands on Java development</strong>, architecture, front end, back end, full stack</li><li><strong>6 years of Android </strong>mobile development and architecture with multiple apps in the app store</li><li><strong>16 years of experience in SQL </strong>7-2016, MySQL, Oracle, and other databases T-SQL, SSIS, SSRS, SSAS, OLTP, OLAP, Multidimensional Cube, MDX, PowerPivot, Tabular Model, SharePoint, PerformancePoint.</li><li><strong>Demonstrated experience and understanding</strong> of the best practices in all aspects of data modeling, data warehousing (<strong>Inmon/Kimball</strong> approach). Solid experience in Data Warehouse</li><li>Strong knowledge and proven results in Data Warehouse and <strong>Data Mart design including Dimensional Modeling (Star &amp; Snowflake Schemas)</strong>, ER Modeling, 3 Normal Forms, Normalization and Demoralization, Logical Model and Physical Model, Fact/Dimension/Hierarchy identifications.</li><li><strong>From Business Case to Data Visualization</strong>, I have designed and developed solutions by combining Business Process with Information Technology.</li><li>Firmware embedded programming, ARM, PIC, DSP, FPGA, RTOS Linux etc.</li><li>Significant management experience including <strong>4 years as the VP of engineering</strong></li></ul><p>EXPERIENCE</p><p>Microsoft, New York, Newport Beach California	3/2019-Present</p><p>This was my 4<sup>th</sup> contract with Microsoft as a high level solution architect and expert. <strong>Please note I am always hands on, always in addition to doing the architecture I do coding, do</strong> <strong>DevOps, and do my own developments and POCs.</strong> This was a complex hybrid multi projects with Kubernetes, Big Data, Multi-cloud (Azue, AWS and GCP), MapR, Kafka, Confluent, SOLR, Elasticsearch etc. It was for an important Microsoft client, a multibillion dollar company in New York with Petabytes of data, and 10 terabyte streaming and data ingestion a day:</p><p>I was involved with multiple Kubernetes projects, a hybrid of MapR and multi-cloud system with a massive MapR cluster (with 1400 note 16000 CPUs) and Multi-Cloud project (Azure AWS and GCP) with cloud Migration, Kafka Confluent streaming using Kappa with massive amount of data in Petabytes with real time streaming using Kappa architecture. This project among others included a comprehensive Data Pipeline, multiple Data Lakes (Landing Zone, Processing Zone and Consumption zone), high level of domain Join security, Big Data Visualizations, Machine Learning, Deep Learning (CNN RNN). The architecture that we created is similar to Uber Gen-4 architecture with Hudi capable of processing tens of petabytes of streaming data. Please note that due to confidentiality and sensitivity I cannot and will not reveal specific technical details.</p><ul><li>I lead, architected and helped developing multiple Kubernetes projects on multiple large Kubernetes clusters. Initially we created POCs on AWS (EKS) and Azure (AKS) but finally decided to use the Google Kubernetes Engine (GKE) in GCP for a number of reasons (more than welcome to ask me why in the interview). We created multiple clusters and deployed HDFS Spark in Kubernetes (up to 1000 pods), Kafka Confluent in Kubernetes (created both but decided on using pure Kafka up to 30 pods brokers in Kubernetes), Zookeeper in Kubernetes (up to 30 pods ZK), a comprehensive SOLR cluster in Kubernetes (50 SOLR pods), Elastic Search in Kubernetes (50 pods Elasticsearch) and other orchestrations. Please note that due to the StatefulSet nature, these projects were fairly complex using headless services for HDFS, Kafka, Zookeeper, SOLR, Elasticsearch etc however after deployments they were very reliable, scalable. For example with one replica command change in Helm Chart it would go from 5 node to 30 brokers Kafka in minutes. Could index from inside the dataproc into the SOLR kubernetes in a lightning speed. Used other component of the GCP among others Cloud Dataproc, BigQuery, Cloud Dataflow, Cloud Data Fusion, Cloud Dataprep, Data Catalog, Google Kubernetes Engine (GKE), Kubernetes CLusters, Dataproc CLusters, Container Registry, CI/CD (CICD, CI CD,CI-CD) pipelines, Deep Learning Containers, Cloud Bigtable, Cloud SQL, Firebase Realtime Database. For artificial intelligence used AI building blocks, Text-to-Speech, Speech-to-Text, AutoML, Vision AI, Cloud Natural Language, Video AI, AI Platform, AI Hub and AI Platform Deep Learning VM Image etc.</li><li>I lead, architected and helped developing a massive MapR cluster with 1400 note 16000 CPUs. A comprehensive Data Pipeline with data ingestions from variety of sources RDBMS, hardware logs, images, streaming data, files, documents into the Data Lake. Using Confluent, Kafka Confluent both inside Kubernetes and outside, Kappa architecture with schema evolution, Avro, Parquet (similar to Uber Gen 4 architecture) we were able to ingest tens of terabytes of data a day like a charm. The Big Data part used Hadoop, Spark, and Kafka Confluent on mainly MapR but also interacting with Azure HDInsight, AWS EMR, and Kubernetes Cluster. Used also, Spark, Spark Streaming, Storm, Kafka Confluent, Hive, Pig, Impala, Flume, Sqoop, MapReduce, Pig, HBase, NiFi, oozie, Tableau Power BI and, Oozie, QlikView etc. Most of the code were written in Python, PySpark and Scala.</li><li>I lead, architected and helped developing a comprehensive Azure infrastructure working with ADF Azure Data Factory gen2, ADLS Azure Data Lake gen2, Data Catalog, Databricks, Delta Lake, Delta tables, Databricks Staging and Merge! Kubernetes, HDInsight, HDInsight Monitoring 4.0, ESP. AD. AAD-DS. ESP, MFA, IR Integration Runtime Gateway, CI/CD (CICD, CI CD,CI-CD) pipelines, Domain join, Apache Ranger, Ambari, Advanced 4 pillar of security for HDInsight (perimeter VNET, Kerberos AD ESP authentication, Hive policies Ranger and data encryption), Advanced ADLS data lake structures, advance Machine learning and CNN, ARM templates, Azure Runbook, notebooks Zeppelin, Jupyter,, Hadoop, Spark, Kafka Confluent, Hive, Hbase. For AI used Azure Machine Learning Service, Azure Machine Learning Studio. For monitoring used Azure Monitor, Azure DevOps, Azure CLI, Azure PowerShell, and Azure Automation. For security I used Azure Active Directory, Azure Role Based Access Control, Azure Subscription Management Azure RBAC, Multi-Factor Authentication, Azure Active Directory Domain Services AAD-DS, Azure Policy, Azure Service Principal, Azure Keys and Key Vault, Enterprise Security Package Azure ESP. For Kubernetes and Micros Services I used Azure Kubernetes Service (AKS), Service Fabric Mesh, Azure Container Instances and Azure Container Registry etc.</li><li>I lead, architected and helped developing a comprehensive AWS infrastructure with: Redshift (also tested POC in Snowflake Data Warehouse and Snowpipe), RDS, EMR, MSK (Kafka Confluent), S3 AWS Compute E2C, Glue, DMS, Athena, EC2, RDS Aurora MySQL PostgreSQL, Elasticsearch, Lambda projects with full Big Data, Amazon Elastic MapReduce (EMR), CI/CD (CICD, CI CD,CI-CD) pipeline, CloudFormation CFN,Spark, Kafka Confluent, Oozie, Sqoop, NiFi, Pig, Hive, Hbase, MSK, AWS CLI, Amazon EMR File System (EMRFS). Comprehensive CDK and CloudFormation experience with writing script in TypeScript, JavaScript, Python, Java, and C#. Collaborative notebooks Zeppelin, Jupyter. AWS networking used VPC, Private Subnet, Public Subnet, Internet Gateway, Routing Security groups etc. Visualization and analysis used QuickSight and CloudSearch. Also created a Kubernetes cluster using Elastic Container Service for Kubernetes (EKS), App Mesh, EC2 Container Service (ECS), FarGate. For AI used deep learning frameworks like Apache MXNet Machine Learning and Deep Learning development and deployment, VMware, AWS Developer Tools, AWS Management Tools, Amazon Machine Learning, SageMaker, Alexa Skills Kit, AWS DeepLens, Amazon Deep Learning AMIs, Amazon TensorFlow on AWS and other components. For security used Identity and Access Management (IAM), AWS Organizations, Multi-Factor Authentication. etc.</li><li>I lead, architected and helped developing multiple  CDK and CloudFormation projects with writing script in TypeScript, JavaScript, Python and Java.</li><li>I lead, architected and helped developing deep learning, machine learning and Convolutional Neural Networks (CNN) using Python, R, PySpark and Scala libraries like scikit-learn, Pandas, Deeplearning4j, Sparkling Water ML, Caffe2, MxNet etc. Different algorithms were used like K-Means, Random Forest, Gradient Boosting algorithms (GBM, XGBoost, XGBoost and CatBoost). Also used GPUs especially with Convolutional Neural Networks CNNs with TensorFlow, Keras.</li><li>Development in, Scala, SBT, Eclipse, Python, Pyspark, R, Zeppeline, Jupyter</li></ul><p>Ericsson, Santa Clara, California	8/2018-3/2019</p><p>Designed and developed data engineering solutions as a senior Hands-On Solution Architect Consultant, Big Data, Data Engineer, Data Scientist, senior developer, Data Warehousing, Spark, HDFS, Kafka Confluent, BI, Kubernetes, IBM Cloud and IBM Cloud Private (ICP), advanced Machine Learning Deep Learning and Convolutional Neural Networks CNN. I am one of the main Architects of GAIA (Ericsson Global Artificial Intelligence Accelerator) a $75,000,000 / year new department intended to use Artificial Intelligence ML, DL, CNNs to revolutionize cellular communication especially for the G5. Working as senior Solution Architect Consultant, senior developer, senior data engineer, senior data scientist. Implemented 2000 node 16,000 CPUs Kubernetes cluster and implemented variety of Machine Learning, Deep Learning and Convolutional Neural Network CNN. The Kubernetes Cluster that I created had Data Pipeline, Data Lake (Raw Data Landing Zone, Processing Zone, Consumption Zone) with the capacity of Peta Bytes 150TB /Day Data for Machine Learning, Convolutional Neural Network CNN, Spark, HDFS, Kafka Confluent, Elasticsearch Logstash Kibana ELK, NiFi, Hive, HBase, Jupyter Zeppelin with Scala and Python. Also implemented advanced Machine Learning, Deep Learning, and Convolutional Neural Networks CNN and deployed massive POCs on Azure Cloud, GCP, AWS and OpenStack with impressive results.</p><ul><li>Migrated and architected a <strong>Multi-Cloud</strong> solution using <strong>AWS</strong> and <strong>Azure </strong>using resources such as Redshift, RDS, EMR, Kinesis, S3, Glue, DMS, Athena, EC2, Lambda projects with full Big Data, CI/CD (CICD, CI CD,CI-CD) pipelines, Amazon Elastic MapReduce (EMR), Hadoop, Spark, Hive, Pig, Kafka Confluent, VPC, Subnet, Gateway, AWS Management Console, AWS CLI, Amazon EMR File System (EMRFS), collaborative notebooks Apache Zeppelin, Jupyter, deep learning frameworks like Apache MXNet, Elasticsearch and SOLR, Machine Learning and Deep Learning development and deployment. AWS Compute E2C, RDS Aurora MySQL PostgreSQL, FarGate, Lambda, VMware, AWS Developer Tools, AWS Management Tools, Amazon Machine Learning, AWS DeepLens, Amazon Deep Learning AIMs, Amazon TensorFlow on AWS and other components. etc.</li></ul><p>In a course of less than a year, I have architected and lead some of the most sophisticated Big Data, Deep learning, Machine Learning, and Convolutional Neural Network CNN in the nation for Ericsson Artificial Intelligence Accelerator (Ericsson GAIA) in Santa Clara California. For the first time the technologies were right to create a comprehensive 2000 nodes 16,000 CPUs cluster on <strong>Kubernetes </strong>with all necessary micro services with automatic orchestration with dynamic deployment for a petabytes Artificial Intelligence system. Some of the results were are impressive. Examples:</p><ul><li>Machine Learning, Deep Learning, Convolutional Neural Network CNN, Anomaly detection on massive amount of real time streaming 5G wireless data with 150TB /day live with high accuracy using the 2000 nodes 16,000 CPUs Kubernetes cluster. This would have taken years of processing in the past and is now feasible in minutes on live data.</li><li>High accuracy at detecting hack attack, security breach and data breach on live data using CNNs</li><li>High accuracy at predicting system availability and reliability and predicting anomalies.</li></ul><p>***Please note that due to the sensitivity and proprietary nature of these projects and since I am one of the main architect I cannot and will not reveal and will not go into too much details!!!</p><ul><li>Created massive Kubernetes clusters on OpenStak, Azure, GCP, AWS with micro service Successfully installed and deployed clusters with more than 2000 nodes 16,000 CPUs deployed in 20 minutes with both Data Pipeline, Data Lake (Raw Data Landing Zone, Processing Zone, Consumption Zone) for Machine Learning, Convolutional Neural Network CNN, Spark on Kubernetes, HDFS on Kubernetes, Kafka Confluent on Kubernetes, Elasticsearch Logstash Kibana ELK on Kubernetes, NiFi on Kubernetes, Hive, HBase, kubeflow, Kube-scheduler, Jupyter Zeppelin with Scala and Python on Kubernetes. Used Vagrant Terraform HashiCorp for deployment with 2000 nodes, 16,000 CPUs with Peta Bytes and 150TB / day capacity.</li><li>I lead, architected and helped developing deep learning, machine learning and Convolutional Neural Networks (CNN) systems for 5G wireless data anomaly detection, system availability, Hack attack, Security breach, data breach detection and protection. For peak performance a distributed architecture were created using 16,000 CPUs with amazing results using Spark, HDFS, Scala, SBT, MLlib, TensorFlow, Keras. Some of the development were based on Multilayer perceptron classifier (MLPC) which is a classifier based on the feedforward artificial neural network. Also created other prototypes using Python, R, PySpark and Scala libraries like scikit-learn, Pandas, Deeplearning4j, Sparkling Water ML, Caffe2, MxNet etc. Different algorithms were used like K-Means, Random Forest, Gradient Boosting algorithms (GBM, XGBoost, XGBoost and CatBoost). Also used GPUs especially with Convolutional Neural Networks CNNs with TensorFlow, Keras.</li><li>I used TensorFlow, Keras and create similar to YOLO type Convolutional Neural Network on GPUs for detection of different type of anomalies on wireless data. I used the conversion of data to pictures and run it through the Neutral network with amazing results.</li><li>I used Acumos AI platform for certain type of ML and CNN projects with cascading Convolutional Neural Networks. I attempted to create an Artificial Intelligence AI environment to facilitate cascading Convolutional Neural Network using.</li><li>Created Data Pipeline with real time, intermediate and permanent repositories on Kubernetes. By using huge Kafka Confluent clusters with huge partitions inside Kubernetes, the data was gathered in a Round-robin fashion from variety of sources including real hardware, routers, radios, etc. and was brought into the real time repositories. Using Scala SBT Python Jupyter Zeppelin, Spark, Kafka Confluent, NiFi, ELK, Kubeflow, Kube-scheduler etc. the capacity of the Data Pipeline was 150TB /Day but could be easily extended by simple Kubernetes Orchestration scripts.</li><li>Created a Petabytes Data Lake with Raw Data Landing Zone, Processing Zone and Consumption Zone. The Data Lake was specifically designed for ease of use for Artificial Intelligence and therefore the Data Scientist were able to directly access different type of data from real time to intermediate to permanent Data from the Data Lakeâs Consumption Zone for different type of anomaly detection. The processing in the Data Lake was using Delta and Flip to ensure that the data is accessible at any time even during the processing. </li><li>Azure Cloud, I architected, led and did actual implementation of massive Azure Cloud project with extensive full cycle Cloud Azure experience covering the Data Ingestion, Data Transformation and Data Consumption with Machine Learning Deep Learning. This was a massive project on a large scale on Azure which covered a full range of areas including but not limited to Big Data, Machine Learning Deep Learning, Azure Machine Learning Studio, Azure Power BI, Azure Search, and Elasticsearch on Azure development and deployment. Comprehensive deployments of massive Azure infrastructures from inside the Visual Studio 2017 using ARM Templates, Azure Runbook. Deployment of many Azure projects for different companies and here are some of practical!! Hands on!! Component that I have personally used, Azure Spark Hive HBase Hadoop Kafka Confluent Storm ML Services (R Server) HDInsight clusters, Domain-joined HDInsight clusters,, Azure Zeppelin notebooks, Azure Jupyter notebooks, Azure SQL Data Warehouses, Azure Databricks, Azure Data Lake, Azure Data Lake Factory CI/CD (CICD, CI CD,CI-CD) pipelines, Azure Data Lake Storage, Azure Data Lake Analytics, Azure Data Links, Azure Integration Runtime (IR), Azure Data Gateway, Azure Kubernetes Services, Azure Storage Blobs, Azure Active Directory, Azure Service Principals, Azure Security Center, Azure Key Vaults, Azure Virtual Network vnet, Azure Log Analytics, Azure Network Interfaces, Azure Cosmos DB, Azure Cortana Intelligence Suite, Infrastructure as a Service IaaS, Platform as a Service PaaS, Microsoft R Server, NLB, Key phrase extraction Azure search, Unstructured text analytics, Event hub, Streaming, Poly Base etc..</li><li>The Big Data part used Hadoop, Spark on mainly MapR but also Cloudera and Hortonworks, Storm, Kafka Confluent, Hive, Pig, Impala, Flume, Sqoop, MapReduce, Pig, HBase, NiFi, oozie, Tableau Power BI and Cloudera visualization, QlikView etc.</li><li>Development languages, Extensive Scala SBT (My Preference), Java Maven, Eclipse Intellij (my preference), Python, R, Jupyter, PySpark, Ruby and even some, Linux Shell Script, Shell Scripts</li></ul><p>Microsoft, New York, California	3/2018-8/2018</p><p>Senior Solution Architect Consultant, senior developer, Azure Big Data, ETL, various databases, Data Warehousing, Spark, Databricks, HDInsight, BI, advanced Machine Learning Deep Learning. I implemented, architected and a massive Azure Cloud infrastructure with big data, machine learning, deep learning, and Artificial Intelligence Neural Network </p><p>This was a project with Microsoft and I worked as Microsoft expert on Azure in New York, other companies involved were Pragmatic Works and Selective in New York. I was involved from the designing board all the way to complete implementation and production release. Azure despite simplicity has enormous amount of details. Many engineers that I interviewed and some of whom that I worked with, may have known bits and pieces but actually creating clusters and implementing systems on Azure require substantial experience and know how, I have that.</p><ul><li>Azure Cloud, I architected, led and did actual implementation of massive Azure Cloud project with extensive full cycle Cloud Azure experience covering the Data Ingestion, Data Transformation and Data Consumption with Machine Learning Deep Learning. This was a massive project on a large scale on Azure which covered a full range of areas including but not limited to Big Data, Machine Learning Deep Learning, Azure Machine Learning Studio, Azure Power BI, Azure Search, and Elasticsearch on Azure development and deployment. Comprehensive deployments of massive Azure infrastructures from inside the Visual Studio 2017 using ARM Templates, Azure Runbook. Deployment of many Azure projects for different companies and here are some of practical!! Hands on!! Component that I have personally used, Azure Spark Hive HBase Hadoop Kafka Confluent Storm ML Services (R Server) HDInsight clusters, Domain-joined HDInsight clusters, Azure Zeppelin notebooks, Azure Jupyter notebooks, Azure SQL Data Warehouses, Azure Databricks, Azure Data Lake, Azure Data Lake Factory, CI/CD (CICD, CI CD,CI-CD) pipelines, Azure Data Lake Storage, Azure Data Lake Analytics, Azure Data Links, Azure Integration Runtime (IR), Azure Data Gateway, Azure Kubernetes Services, Azure Storage Blobs, Azure Active Directory, Azure Service Principals, Azure Security Center, Azure Key Vaults, Azure Virtual Network vnet, Azure Log Analytics, Azure Network Interfaces, Azure Cosmos DB, Azure Cortana Intelligence Suite, Infrastructure as a Service IaaS, Platform as a Service PaaS, Microsoft R Server, NLB, Key phrase extraction Azure search, Unstructured text analytics, Event hub, Streaming, Poly Base etc.</li></ul><p>Optum United Healthcare, Santa Ana (partially in Minneapolis), California	8/2016-3/2018</p><p>Lead, Senior Solution Architect Consultant, senior developer, Big Data, Data Engineer, Data Scientist, Data Warehousing, Spark, HDFS, Kafka, BI, Kubernetes, IBM Cloud and IBM Cloud Private (ICP), advanced Machine Learning Deep Learning on the cutting edge of the Genome, Berkeley Amplab Adam Genomics, GATK. Elasticsearch SOLR. Also implemented and deployed a massive Azure Cloud infrastructure with big data, machine learning, deep learning, and artificial intelligence convolutional neural network CNN.</p><p>In a course of one year, I have architected and lead some of the most sophisticated Big Data, Deep learning and Machine Learning, Azure Cloud projects in the nation for Optum in Minneapolis and California. I have had access and utilized thousands of servers, 12,000 CPUs, enormous amount of memories and the results has been astonishing beyond even my own and everybodyâs expectations. Examples:</p><ul><li>Genomic analysis for prediction of various cancers on 3000 known samples from our genomic bank, 12,000 CPUs, enormous amount of memories the <strong><em>processing time were reduced from 46.7 years to 22 minutes 47 seconds</em></strong>! with 99% prediction accuracy, this was recently presented at a conference.</li><li>4 billion records with 53 pre and post processing queries, reduced from days to under a minute!</li></ul><p>Please note that due to the highly proprietary and sensitive nature of these projects, I will not be able nor will I disclose the technical details.</p><ul><li>I lead, architected and helped developing deep learning and machine learning systems for genetic analysis and prediction system for occurring of different type of cancers with more than 99% accuracy. For peak performance a distributed architecture were architected using 12000 CPUs with amazing results. Spark, HDFS, Scala, SBT, MLlib, TensorFlow, Keras. Some of the development were based on Multilayer perceptron classifier (MLPC) which is a classifier based on the feedforward artificial neural network. Also created other prototypes using Python, R, PySpark and Scala libraries like scikit-learn, Pandas, Deeplearning4j, Sparkling Water ML, Caffe2, MxNet etc. Different algorithms were used like K-Means, Random Forest, Gradient Boosting algorithms (GBM, XGBoost, XGBoost and CatBoost)</li><li>For hardware processing of the Big Data, the Deep Learning and Machine Learning different architectures were tested on IBM Neteeza, Teradata and distributed architecture with 12000 CPUs and memory (Spark). The result were absolutely clear, there is no comparison the distributed architecture is far more superior and is the future!</li><li>The Big Data part used Hadoop, Spark on mainly MapR but also Cloudera and Hortonworks, Storm, KafKa, Hive, Pig, Impala, Flume, Sqoop, MapReduce, Pig, HBase, NiFi, oozie, Tableau Power BI and Cloudera visualization, QlikView etc.</li><li>I lead, architected and helped developing a hybrid system of Elasticsearch and HDFS using Logstash, Rsync and Kafka. The Elasticsearch was sharded over 25 nodes but later deployed on AWS. Three type of data were indexed and inputted into the Elasticsearch (Kibana):<ul><li>The hardware and system logs for real time (Kafka) hardware and system monitoring</li><li>The patient and claim data from HDFS, Hive and HBase for search and quick BI visualization in Kibana.</li><li>Data Export files from MarkLogic </li><li>For basic search used by different system via an API on top of the Elasticsearch</li></ul></li><li>I lead, architected and helped developing a massive data ingest system from different providers with a permanent and real time pipeline (Kafka) using Delta and Flip methods for an ongoing uninterrupted data ingest. The source data were RDBMS, Hive, HBase, MarkLogic, flat data and log files etc.</li><li>Azure Cloud, I architected, led and did actual implementation of massive Azure Cloud project with extensive full cycle Cloud Azure experience covering the Data Ingestion, Data Transformation and Data Consumption with Machine Learning Deep Learning. This was a massive project on a large scale on Azure which covered a full range of areas including but not limited to Big Data, Machine Learning Deep Learning, Azure Machine Learning Studio, Azure Power BI, Azure Search, and Elasticsearch on Azure development and deployment. Comprehensive deployments of massive Azure infrastructures from inside the Visual Studio 2017 using ARM Templates, Azure Runbook. Deployment of many Azure projects for different companies and here are some of practical!! Hands on!! Component that I have personally used, Azure Spark Hive HBase Hadoop Kafka Storm ML Services (R Server) HDInsight clusters, Domain-joined HDInsight clusters, CI/CD (CICD, CI CD,CI-CD) pipelines, Azure Zeppelin notebooks, Azure Jupyter notebooks, Azure SQL Data Warehouses, Azure Databricks, Azure Data Lake, Azure Data Lake Factory, Azure Data Lake Storage, Azure Data Lake Analytics, Azure Data Links, Azure Integration Runtime (IR), Azure Data Gateway, Azure Kubernetes Services, Azure Storage Blobs, Azure Active Directory, Azure Service Principals, Azure Security Center, Azure Key Vaults, Azure Virtual Network vnet, Azure Log Analytics, Azure Network Interfaces, Azure Cosmos DB, Azure Cortana Intelligence Suite, Infrastructure as a Service IaaS, Platform as a Service PaaS, Microsoft R Server, NLB, Key phrase extraction Azure search, Unstructured text analytics, Event hub, Streaming, Poly Base etc..</li><li>Genome Analysis Toolkit 4 (GATK4) from Broad Institute, ADAM Genomics Berkeley AMPLab, BAM, SAM, VCF genome variant. Worked also with mango, gnocchi, deca, avocado, quinine, cannoli etc</li><li>Using SOLR / Elasticsearch created a detail analytical graphical dashboard in Kibana for Patient Data, Claim Data and Provider Data.</li><li>I architected and led multiple AWS projects, Redshift, RDS, EMR, Kinesis, S3, Glue, DMS, Athena, EC2, Lambda, with full Big Data, Amazon Elastic MapReduce (EMR), Hadoop, Spark, Hive, Pig, Kafka, AWS Management Console, AWS CLI, Amazon EMR File System (EMRFS), collaborative notebooks Apache Zeppelin, Jupyter, deep learning frameworks like Apache MXNet, Elasticsearch and SOLR, Machine Learning and Deep Learning development and deployment. AWS Compute E2C, FarGate, Lambda, VMware, AWS Developer Tools, CI/CD (CICD, CI CD,CI-CD) pipelines, AWS Management Tools, Amazon Machine Learning, AWS DeepLens, Amazon Deep Learning AIMs, Amazon TensorFlow on AWS and other components. etc.</li><li>Development languages, Extensive Scala SBT (My Preference), Java Maven, Eclipse Intellij (my preference), Python, R, Jupyter, PySpark, Ruby and even some, Linux Shell Script, Shell Scripts</li><li>Successfully installed and deployed an entire IBM Cloud Private ICP Cluster then implemented and deployed ELK Elasticsearch, Logstash, Kibana, Filebeat, Kafka, Zookeeper, Cassandra, Curator on ICP IBM Private Cloud, Kubernetes, Pods using Helm Charts, Scala SBT.<strong> </strong>IBM Cloud and IBM Cloud Private (ICP) is a Distributed Container based Architecture, Docker, Docker CLI, Kubernetes, Kubernetes CLI, Pods, Pods deployments. Service Deployments, Ingress, Helm Charts, Helm Charts CLI. </li></ul><p>OneStop, El Segundo, California	2/2016-8/2016</p><p>Senior lead, Senior Solution Architect Consultant, senior developer, Big Data, Data Warehousing, BI, SOLR, Lucene, Elasticsearch, Mahout, Weka Machine Learning Lead.</p><p>I was initially hired at OneStop because of similar experience I had from Dell and Microsoft in Big Data, SOLR Elasticsearch and Machine Learning, taxonomy etc.</p><ul><li>I lead, architected and helped developing a Big Data system with 256 nodes using Hadoop, Spark on Hortonworks and later migrated to Cloudera, Storm, KafKa, Hive, Pig, Impala, Flume, Sqoop, MapReduce, Pig, HBase, oozie, Tableau Power BI and Cloudera visualization, QlikView</li><li>Development languages, extensive Scala SBT (my preference), Java Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Unix Shell Scripts, Linux Shell Scripts</li><li>I lead, architected and help developing the SOLR/Lucene Search system which was later migrated to Elasticsearch with 15 nodes sharding. The system was initially developed and maintained in house but later deployed to AWS cloud and prototyped on Azure. The development and test of the 15 node done on VirtualBox machines, physical machines before deployment to the Cloud. The SOLR development was done in two different phases, initially we did indexing directly on top of the metadata extracted from various files with Apache Tika, Apache Flume and scoop. I wrote a scheduler in Java that run delta indexing periodically every few hours. We had customized faceting and then the API would grab the top N results from the XML. The search worked better than expected, the indexing was slow but the search was extremely fast in fraction of a second. On the second phase we stored all the raw documents in HDFS and create indexing and then use HBase to store the index files in HDFS. Also an API was developed in JAVA with a .NET wrapper with SOLR search calls into the SOLR engine.</li><li>I lead, architected and help developing an advanced Machine Learning system initially in Spark MLlib then Weka and ultimately a Mahout Machine learning and recommendation system using both ItemSimilarity and UserNeighborhood. I personally favored and created porotypes using Spark MLlib, TensorFlow, Keras, Python libraries like scikit-learn, Pandas but in this case Mahout worked very well. The .NET API would record every time a product was clicked or purchased. The data was recorded in the database and then the metadata was created and the mahout would create a scoring table (0-10) for product and region. The .NET API would select top N highest score and would present it as recommendations.</li><li>I lead, architected and helped developing a gigantic amount of data extraction, data warehousing, Big Data. The data was gathered in access of tens of terabytes from more than 40 top of the lines brands Ecommerce sites partnered and operated by OneStop like FRYE, Juicy couture, NYDJ, PAIGE, Splendid, Coffee Beans, Jones New York, Hudson and many more. Used SSIS ETL for SQL to port data to the Data Warehouse and then used Sqoop for extracting from RDBMS to the HDFS, used Flume for extracting from logs files, FTP NAS files to the HDFS, used Apache Tika and Java for extracting metadata from various files into the HDFS, used Nutch for web crawling and for extraction metadata into the HDFS, used SAPI, CMU Sphinx, Kaldi for customer service voice to text conversion into the HDFS</li><li>I architected and implemented a real time and streaming component for the Cloudera visualization using Apache Strom and Apache Kafka.</li><li>I lead, architected and help developing an elaborate real time visualization using Tableau and Cloudera visualization for the big data portion.</li><li>The Big data prototype was deployed both on AWS and Azure. For a number reasons the final decision for the cloud deployment was made for deployment into the AWS not Azure.</li><li>I lead and oversaw the conversion of part of the SOLR search project to Elasticsearch and benchmarked the performance. Although I liked working with JASON for various reason SOLR was preferred initially.</li><li>I lead, architected and help developing Kibana 4.5 visualization on top of both SOLR and Elasticsearch.</li><li>Wrote and oversaw a development of combination of batch files, python and Ruby scripts for SOLR/Lucene and Big Data deployment and configurations. I have to add that I started the conversion of batch file to Python but there were simply not enough time.</li><li>Did extensive prototyping and benchmarking and helped evaluating the performance of the big data on Massively Parallel Processing (MPP) and other  Data Warehouse Appliances such as IBM Netezza, Teradata, APS (PDW), Oracle Exadata</li></ul><p>Canadian Tire, California / Toronto	8/2015-2/2016</p><p>Senior Big Data, DW and BI Lead Solution Architect Consultant.</p><ul><li>Led multiple large scale Big Data, Enterprise Data Warehouse EDW and Business Intelligence BI projects on Teradata utilizing Spark, Hadoop, Hortonworks, Cloudera, Hive, Impala, Flume, Sqoop, Map Reduce, Pig, HDInsight, HBase, oozie, and facilitating the real-time data analysis by the data scientist.</li><li>Led multiple EDW projects, prototyped and evaluated their performance on the Massively Parallel Processing (MPP), other Data Warehouse Appliances such as IBM Netezza, Teradata, APS (PDW), Oracle Exadata</li><li>Development languages, extensive Scala SBT (my preference), Java Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Unix Shell Script, Linux Shell Scripts</li><li>Leading the team, I designed architected and implemented the migrating from legacy information warehouse to a modern high performance Big Data and Data Warehouse running on multiple DW appliances. Drafted a BI/DW prioritized implementation roadmap working with the business and finance department.</li><li>Leading the team we migrated and deployed 5 projects to Azure Cloud. I was personally involved in the full cycle of vendor selection, requirement gathering, design, development and the deployment of these projects. The migration included different aspects of the projects from front, backend, and integration. We went through thorough research before selecting the Azure cloud for this project and also utilized cutting edge utilities to perform the migration and deployment.</li><li>Drafted a BI/DW prioritized implementation roadmap while taking input from internal divisional service plans, business and IT strategy documentation, as well as corporate BI Strategy and the Financial Planning and Reporting System</li><li>Designed Enterprise Information Management (EIM) solutions for retail operation. Led technical teams and designed various BI solutions including loyalty programs, card management, POS data management, customer behavioral analysis, store dashboards, finance, ecommerce, cyber security analytic.</li><li>Defined the data governance strategy, designed security patterns, implemented data standards and procedures across the enterprise; drafted business specific methodology to establish business stakeholder-driven data stewardship through MDM</li><li>Conducted BI maturity assessment of the organization. Architected DW&amp;BI Program Structure, defined the role of DW&amp;BI Program Steering Committee, it's mission, objectives, roles and responsibilities, monitored regular improvements to help manage risks, evaluate trends, and develop capacity and capability to achieve the Program mission</li></ul><p>NovaWurks/DARPA, Los Alamitos, California	11/2014-8/2015</p><p>Senior Big Data, DW and BI Lead Solution Architect Consultant, Java Android consultant</p><p>Worked as senior Big Data Solution Architect, team leader and core developer on PHOENIX project, an advanced satellite system for DARPA (Defense Advanced Research Projects Agency), a network of small satellites due to launch to orbit in 2015. Due to the sensitivity cannot go into too much details!</p><ul><li>Led several Big Data projects on massive amount of transmitted and logged data from the satellite network to the ground station. These projects were developed utilizing Cloudera, Hadoop, Spark, Hive, Impala, Flume, Sqoop, Storm, Pig, HDInsight, HBase, oozie. Due to the real time nature of the project Apache Storm and Apache Kafka was used for handling of the streaming and the real time data feed.</li><li>I led the team, designed, architected and implemented an elaborate Data Warehouse and Data Mart using Dimensional Modelling Star Schema for satellite data aggregation, data storage, data log, real time operation status data and other needs.</li><li>Utilized the Cloudera Visualizations, Dashboards, and Reports to monitor the operation of the satellites and any warning issues due to any errors, miss functions or failures. Other visualization tools were also created using Java and Android.</li><li>Development languages, extensive Scala SBT (my preference), Java Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Shell Scripts</li><li>Led the team developed multiple real time Android Apps and middleware using Android Studio and Eclipse, Android SDK and Java, RESTful APIs, Retrofit, GSON, JSON, Regex, JGroups IP Multicast, Apache Thrift, Python. Also used the following technologies and systems, Xilinx FPGA, Verilog, TI DSP, ARMÂ® CortexÂ®-A9 Cores: i.MX 6 Series Multicore Processors etc.</li></ul><p>Paramit, Morgan Hill, California	7/2013-11/2014</p><p>Senior Big Data, DW and BI Lead Solution Architect, .NET Architect Consultant.</p><ul><li>Led multiple large scale Big Data, Enterprise Data Warehouse EDW and Business Intelligence BI projects utilizing, Hadoop, Cloudera, Hive, Impala, Flume, Sqoop, Map Reduce, Pig, HDInsight, HBase, oozie, and facilitating real-time data analysis by data scientist.</li><li>Leading the team, I designed architected and implemented the migration from legacy normalized SQL, FoxPro, medical device manufacturing, ERP, MRP, CRM, sales, finance and other information warehouses to a consolidated modern high performance Big Data Warehouses running on multiple DW appliances.</li><li>Development languages, extensive Scala SBT (my preference), Java Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Shell Scripts</li><li>Leading the team we migrated and deployed multiple projects to Azure Cloud. I was personally involved in the full cycle of vendor selection, requirement gathering, design, development and the deployment of these projects. The migration included different aspects of the projects from front, backend, and integration. We went through thorough research before selecting the Azure cloud for this project and also utilized cutting edge utilities to perform the migration and deployment.</li><li>Using a combination of WPF C# application GUI and the Cloudera Visualizations, Dashboards, and Reports created advanced data visualization and data entry tools for ERP, MRP, CRM, sales, finance and other departments.</li><li>I lead, architected and help developing a SOLR/Lucene Search for the huge amount of ERP, MRP and CRM. The SOLR project was later converted to Elasticsearch. The Elasticsearch /Lucene system was architected with 5 nodes sharding. It was developed and tested on 5 node VirtualBox machines and then deployed to AWS cloud. Created an API in C# .NET for calls to the search engine. Also a GUI was developed in C# .NET for search calls to the Elasticsearch.</li><li>Developed a customized SOLR indexing scheduler in C# which would run periodically to do the delta indexing. </li><li>Drafted a BI/DW prioritized implementation roadmap while taking input from internal divisional service plans, business and IT strategy documentation, as well as corporate BI Strategy and the Financial Planning and Reporting System</li><li>Designed Enterprise Information Management (EIM) solutions for the manufacturing process, customer support and retail operation. Led technical teams and designed various BI solutions including medical device manufacturing tracking process, component reliability analysis, vendor analysis, customer behavioral analysis, finance, ecommerce, cyber security analytic.</li><li>Conducted BI maturity assessment of the organization. Architected DW&amp;BI Program Structure, defined the role of DW&amp;BI Program Steering Committee, it's mission, objectives, roles and responsibilities, monitored regular improvements to help manage risks, evaluate trends, and develop capacity and capability to achieve the Program mission</li><li>Led the team and developed multiple applications including medical device, ERP, MRP applications with big data architecture. Used NET 4.5, C#, WPF, WCF, WF, MVVM Light, Telrik, MVC 4 Razor Entity Framework 6.0 TFS, SQL 2012.</li></ul><p>Microsoft, Redmond WA	2/2013-7/2013</p><p>Senior Big Data, DW and BI Lead Solution Architect, .NET Architect Consultant.</p><ul><li>Led multiple Azure Cloud Big Data, NoSQL Riak, MongoDB SIP Trunk VOIP projects doing analysis on massive amount of voice to text converted data utilizing Hadoop/HDInsight, PDW, Map/Reduce jobs, Hive, and Sqoop.</li><li>Development languages, extensive C++, Scala SBT (my preference), Java Maven, Eclipse Intellij, Python, R, PySpark, Ruby, C#, Shell Scripts</li><li>Created real time multithreaded C# code using C++ Dubango Library, SIP, TCP, UDP, RTP the VOIP telephony voice was recorded and using SAPI converted to text. The text was then stored into key value and document tables using Riak and MangoDB. The voice data gathered from Cisco/IPCC telephone systems. Integrated with Cisco Verint for VOIP call recording, quality monitoring (QM), and speech analytics.</li><li>Microsoft SQL Server Parallel Data Warehouse (SQL Server PDW) was chosen as the main appliance for the Big Data processing due to its Massively Parallel Processing (MPP) architecture designed for Big Data Processing.</li><li>Microsoft Power BI in conjunction with a .NET application is used for data visualization.</li><li>Led the design and development of the Workforce Management (WFM) data warehouse and BI solution to optimize adherence and attendance in the contact center. The predictive analytic component accurately forecasts the number of CSRs needed in the call center to fulfill the services. </li><li>Led the design and development of an efficient BI auditing framework that collects the data from packages being executed and used in data flows, row counters, versioning, and error handling. The framework is crucial for monitoring, timing, troubleshooting, and auditing. Also, developed Stored Procedures, Views, and Functions for the framework to automate logging the information and error handling in the packages.</li><li>Led the design and development of ETL processes and data mapping using SQL server, Master Data Services (MDS), SSIS to extract data from Lagan ECM and division data sources including SQL server and oracle databases, flat files, and excel sheets. The data, then, is transformed and loaded into a data warehouse for reporting.</li><li>Led the design and development of data quality ETL packages to correct and cleanse the data and enhance the quality of consolidated data. Wrote hundreds lines of .NET C# code, embedded in the packages, to create a rules engine that loads business rules and apply them to the data efficiently. In addition, the data quality issues are mapped for reporting purposes. </li><li>Led the design and development of a SQL Server SSAS Analysis cube utilizing star schema with complex MDX calculated measures, named sets and KPIs to present an analytical view for the data and data quality with multiple dimensions.</li><li>Led the design and development of map application and report using ASP.NET/C# web application. The application loads the data from the data warehouse, combines it with geographical information, and displays the data on a map. The application communicates through restful mapping services and uses client side scripts (JavaScript and AJAX) to improve performance and user experience.</li></ul><p>Dell, Austin, TX	6/2012-2/2013</p><p>Senior Big Data, DW and BI Lead Solution Architect, .NET Architect Consultant.</p><ul><li>Led a Big Data project on gigantic amount of taxonomy data and customer portfolio using Hadoop,  Cloudera, Hive, Map Reduce, Pig, HDInsight, and facilitating real-time data which was both analyzed and also in real time restructured the Dell website on the demographic portfolio of the customers.</li><li>I architected, worked and help developing the SOLR/Lucene Search deployed to Azure. The indexing was done directly on top of the metadata extracted from various files with customized Java code and Apache Tika. Used customized faceting to overwrite the default search criteria.</li><li>Development languages, extensive Java Maven, Eclipse Intellij, Python, R, Scala SBT, PySpark, Ruby, C#, Unix Shell Scripts</li><li>Developed a customized SOLR indexing scheduler in C# which would run periodically to do delta indexing. </li><li>Wrote variation of batch files, python for SOLR/Lucene deployment and configurations</li><li>Leading the team, we designed architected and implemented the migrating from legacy normalized SQL taxonomy data, customer portfolio data and other data to a modern high performance Big Data Warehouses running on multiple DW appliances.</li><li>Defined the data governance strategy, designed security patterns, implemented data standards and procedures across the enterprise; drafted business specific methodology to establish business stakeholder-driven data stewardship through MDM</li><li>Led multiple EDW projects, prototyped and evaluated the performance on Azure cloud, AWS Amazon Cloud, Massively Parallel Processing (MPP) Data Warehouse Appliance</li><li>I wrote complicated taxonomy algorithm in C# to load, sort the taxonomy data into huge multidimensional trees on the memory which made the data processing supper fast.</li><li>Created Taxonomy data visualization using the Cloudera Visualizations, Dashboards, and Reports to monitor customer profile, demography and other useful data. Other visualization tools were also created using C#.</li><li>Created data quality ETL packages to correct and cleanse the taxonomy data and enhance the quality of consolidated data. The consolidated taxonomy data then were segmented using Hadoop and Cloudera.</li><li>Led the design and development of a SQL Server SSAS Analysis cube utilizing star schema with complex MDX calculated measures, named sets and KPIs to present an analytical view for the data and data quality with multiple dimensions.</li><li>Leading the team we migrated and deployed multiple projects to Azure Cloud. I was involved in the full cycle of vendor selection, requirement gathering, design, development and the deployment of these projects. The migration included different aspects of the projects from front, backend, and integration.</li><li>In conjunction with the Big Data I was involved in multiple projects using variety of technologies including MVC 4 Razor, WPF, WF, WCF, TPL, LINQ, SQL 2012, jQuery, Android, Java, J2EE, JRE, Ajax, AngularJS, ExtJS, Entity Framework 5.0,.NET 4.5</li></ul><p>BEW / General Electric / 3 Gorges China, San Ramon, California	6/2011-6/2012</p><p>Team Leader, .NET Architect, Hands on Developer Consultant.</p><ul><li>Worked as a system architect, core developer on a sophisticated control system for generators and wind turbines lead the software (WPF), hardware (Xilinx FPGA &amp; TI DSP 6000) and firmware (C++ Verilog/VHDL) teams.</li><li>The high level software controlled a network of generators via TCP/IP. The WPF C# project was architected using MVVM light, Entity Framework, LINQ, WCF Services SQL etc. The Silverlight ASP .NET project was architected using MVVM light, Entity Framework, LINQ, WCF RIA Services Domain Service/Context. Developed equivalent Android application for reading the generatorâs parameters like RPM, temperature, sensor Voltages etc. Used Java programming and the Android Software Development Kit, Eclipse using the Android Development Tools (ADT) Plugin. Also worked on the firmware and FPGA DSP TMS320C6713 TMS320F28335 EMIF, I2C, MCBS, GPIO, RTC UART, Anybus CANbus, DM9000, second level bootloader, EEPROM, code composer 3.3 etc FPGA Xilinx Spartan 6, Xilinx ISE Design Suite 13.2, Verilog and VHDL.</li></ul><p>Texas Instruments, Dallas Texas	9/2010-6/2011</p><p>Team Leader, .NET Architect, Hands on Developer Consultant.</p><p>Worked as the main architect, team leader, and core developer on a scientific highly multithreaded WPF C# application for emulation and design of advanced communication chips using scientific algorithms. I also worked on an Android application for the PLL, Java programming using the Android Software Development Kit, Eclipse using the Android Development Tools (ADT) Plugin. The WPF application was architected using propriety MVVM architecture. Utilizing advanced 3D objects the application was similar to OrCad and AutoCad. A smaller prototype version was also developed in Silverlight.</p><p>Multibeam Corporation / Tokyo Electron (TEL), Santa Clara, California	7/2008-9/2010</p><p><strong>Team Leader, Architect, Hands on Developer Consultant.</strong></p><ul><li>Worked as the project lead, helped designing, architecting, and implementing a revolutionary complex electron beam based instrument for the next generations of semiconductor fabs. Advanced analog digital boards, Embedded Linux, Xilinx &amp; Altera FPGA, Quartus, NIOS, ARM9, ARM11, C, C# .NET, WPF (MVVM), WCF etc. DSP TMS320C67x GPIO, RTC UART, Modbus, DM9000, second level bootloader, EEPROM. Altera FPGA, Stratix, Cyclone Series, Quartus II Nios II.</li></ul><p>Department of Defense Contract (DOD), Washington DC	3/2007-7/2008</p><p><strong>Team Leader, Architect, Hands on Developer Consultant.</strong></p><ul><li>Architected, developed and led a highly sophisticated hardware/firmware/software system. Due to the classified nature of the project, I can only provide the following generic information: The project involved advanced radio scanners, signal generators using GPS, WCDMA, CDMA, GSM and other systems and protocols. The software application controlling the instruments was a multi-tiered application written in C#, .NET, Visual C++, MFC, CLR, Embedded Linux. It utilized a very advanced multi-threading architecture with sophisticated synchronization, message handling, logging system, serialization etc. Specialized algorithms were devised to speed up the real-time performance of hardware/software. Again because of the defense-related nature of this project I cannot reveal any more details.</li></ul><p>Patton Design, Irvine, California	7/2000-3/2007</p><p><strong>Vice President.</strong></p><p>Worked as the vice president of software and hardware. I led and developed the software/hardware for a $140,000 instrument medical device - FDA. Please check the website of Patton Design and Busch &amp; Lomb to see this award winning instrument for cataract surgery. I designed, architected, led the team and developed the software and also directed the hardware and firmware developments. The software included a sophisticated multithreading architecture, RS232 and TCP/IP communications, managed wrapper for firmware calls, video streaming, voice recognition, database hierarchy encryption etc. In addition to leading the team and acting as the vice president, I personally wrote the complex core components in C# .NET. Due to the large scale of the medical device - FDA projects with hundreds of screens many of the .NET C# libraries and objects had to be used. We also used legacy unmanaged code inside the managed code (wrappers). DirecX, DirectShow, Windows Communication foundation WCF, Windows Presentation Foundation WPF, WF, Silverlight, WCSF, SCSF, Enterprise Library, animation, video, audio etc were also used.</p><p>In addition to the main control application I wrote and oversaw the firmware in C++ Embedded Linux, C++ Round Robin, CodeWarrior. I also oversaw and participated in the hardware development using OrCad 10.</p><p>* <strong>Patton Design / Cameron Health:</strong> Developed the software and participated in the hardware design of the heart pacemaker medical device - FDA and the controller called <strong>Q-TECHâ¢ Programmer.</strong> medical device - FDA<strong> </strong>The heart pacemaker is transplanted in the heart and controlled by the wireless controller via Bluetooth. Due to the FDA regulations I could not use the .NET framework but had to use Embedded Visual C++ 4.0 and MFC for windows CE. More than 140 screens! Very sophisticated programming involving memory managements, DirectX, DirectShow etc.</p><p>* <strong>Patton Design / GoVideo:</strong> Worked as the Vice President/architect/team leader on a joint project between, GoVideo, Patton Design, Daewoo and MTK in Taiwan. I led the Patton Design team developed a TiVo style DVD/VCR combo with hard drive recording capability. I was the vice president and the team coordinator between the 4 companies overseeing hardware, software and Firmware (Embedded Linux), several patents were filed. The System was presented at the CES show in Las Vegas in 2007 and received tremendous positive recognitions.</p><p>* <strong>Contract</strong> with <strong>usCalibration Inc.: </strong>Architected, developed and led a sophisticated web based application using C# .NET and Visual Studio 2005, SQL Server 2005 and SSRS. I wrote the core part of the application. The application was successfully launched in 2006 for Calibration systems with advanced security systems. Tens of thousands of lines of code with advanced navigation systems with several pending patents.</p><p>Hewlett Packard (HP), Cupertino, California	4/2000-7/2000</p><p><strong>Team Leader, Architect, Hands on Developer Consultant.</strong></p><p>Worked as senior developer/ technical lead on an advanced server client based communication system for server diagnostics. The system was designed using TCP/IP and SNMP protocols for monitoring hardware sensors like thermocouples, voltage and current monitoring sensors and other hardware sensors installed on HP servers. By reading these sensors, HP was able to remotely do detailed hardware/software diagnostics of the HP servers around the globe.</p><p>Worked on hardware, software and the overall system architecture. The software had a server and client component and was written in visual C++, COM (ATL), DCOM, ASP, Visual J++, XML, SNMP, MIB, SQL and InstallShield.</p><p>Broad Logic, Milpitas, California	7/99-4/2000</p><p><strong>Team Leader, Architect, Hands on Developer Consultant.</strong></p><p>I was brought to BroadLogic, Inc. by Paul Rudnick because of my expertise in satellite communication systems and my experience from Space Systems Loral and CyberStar. Prior to this, I had worked closely with Adaptec and Broadlogic on the development of the satellite receiver hardware while still a senior manager at Space Systems Loral. </p><p>I worked on the design and implementation of the next generation of two way satellite Express PC transceiver cards, a high speed two way satellite communication system. I designed, simulated, researched, architected and led the project for the development of an advanced two way satellite communication system (satellite Express PC transceiver cards). Audio, video transfer and high speed internet access over satellite. Using, frequency, time, phase multiplexing. TDMA, CDMA, GMSK, Conditional Access. TCP/IP, UDP, DVB, SNMP, MIB and proprietary protocols. Using OQPSK modulation implementation on the Texas Instrument DSP Chip. I have written several documents related to this system.</p><p>Hewlett Packard (HP), Mayfield, California	3/99-7/99</p><p><strong>Team Leader, Architect, Hands on Developer Consultant.</strong></p><p>Worked as the senior architect, technical lead and senior developer on the HP Ecommerce site which later became the foundation of the HP website for PC and servers. The web application was developed in Visual InterDev 6.0 using Active Server Pages (ASP), Microsoft E-Commerce, SQL 7.0, XML, Visual C++ 6.0 and Visual Basic 6.0, Visual J++ 6.0, COM (ATL), DCOM, JavaScript and VB Script. The web server was Microsoft Internet Information Server (IIS), Microsoft site server 3.0, with Microsoft E-Commerce edition 3.0 and FrontPage extension running under the NT Server I have written several documents related to this application.</p><p>Hewlett Packard (HP), Cupertino, California	5/98-3/99</p><p><strong>Team Leader, Architect, Hands on Developer Consultant.</strong></p><p>Worked as the senior architect, technical lead and senior developer on the HP servers configuration software which later became a major component and the foundation of the HP website for PC and servers configuration. Stand alone and the web application in was developed in Visual InterDev 6.0 using Active Server Pages (ASP), Microsoft E-Commerce, SQL 6.5, Visual C++ 6.0 and Visual Basic 6.0, COM (ATL), DCOM, JavaScript and VB Script. The web server was Microsoft Internet Information Server (IIS), Microsoft site server and FrontPage extension running under the NT Server. I have written several documents related to this application.</p><p>Space Systems Loral (CyberStar), Mountain View, California	11/96- 5/98</p><p><strong>Team Leader, Architect, Hands on Developer Consultant.</strong></p><p>I was brought into Space Systems Loral from Lockheed Martin by Bob Lapin to help starting the CyberStar division at the Space Systems Loral. By the time I left the CyberStar in 1998 to finish my PhD in Satellite Communication, the CyberStar division had grown to more than 100 employees. I personally interviewed majority of those people. </p><p>I was one of the main architects of the CyberStar project and oversaw the design, development and implementation of different aspects of hardware, software, firmware and the satellite communication at CyberStar.</p><p>I first established a complete satellite communication link both uplink and downlink, using 3rd party modulators, demodulators, encoders, decoders, cryptography modules, conditional access, transmitter, receivers, amplifiers, dampers, data aggregator, data parsers etc. Very soon we were able to transmit and receive from and to the satellite. We were primarily using MIB and DVB protocols initially but I was one the first who managed in 1998 to implement TCP/IP and high speed internet access over satellite using an ACK table!! (patents)</p><p>To develop the integrated transceiver hardware we started working with Adaptec and I personally was directly involved in the design and implementation of the satellite receiver card hardware using OrCad. This later led to the creation of BroadLogic from Adaptec. I was later hired by BroadLogic to continue the improvement of the two way satellite receiver / transmitter.</p><p><strong><em>This project was personally very important to me and made me understand and experience the satellite communication in a very comprehensive way both theoretically and practically. It helped me to get a PhD in Low Earth Orbit Satellite Communication from the University of California, one of very few who did. I travelled extensively in both US and in Europe and came in contact with some amazing people from NASA, Lockheed Martin, BroadLogic, European Space Agency etc I wrote many documents in satellite communication during this period for Space Systems Loral.</em></strong></p><p>Lockheed Martin, Milpitas, California	6/96- 11/96</p><p><strong>Team Leader, Architect, Hands on Developer Consultant.</strong></p><p>Lockheed Martin at the time in 1996 had the most sophisticated high resolution CCDs (Farichild) in the world which were in use in a number of sensitive military applications, advanced high resolution digital satellite imaging, and few civilian applications. </p><p>Due to the classified nature of some of these projects I cannot in detail describe what I did. However I was involved in the design and development of some of these advanced and sensitive projects. I worked as a senior engineer, designing and developing, systems, hardware, firmware and software.</p><p>* Hardware: We used OrCad for designing analog and digital circuits, filters, amplifiers, Data collectors from CCDs, interfaces etc.</p><p>*Firmware was written in C++ , flat file Round Robin, on Freescale HC and ARM family CPUs.</p><p>*Software: developed 32-bits, real time applications in Visual C++ 4.2 using MFC and SDKs under Windows 95 for control and testing of an advanced digital camera with high resolution CCD. The GUI software is designed for driving the special digital camera through parallel communication and testing of IPS, ADP, CCD and different part of the system. The tests included advanced image processing and image quality tests. The project involved both 16-bits and 32-bits DLLs and VXDs (device drivers), Thunking and also conversion from and between 16-bits and 32-bits.</p><p>OTHER PROJECTS</p><p>Ericsson (Ellemtel), Stockholm, Sweden	</p><p><strong>Team Leader, Architect, Hands on Developer.</strong></p><p>Developed and designed hardware and a control system for the new generation of AXE telephone systems, based on the FUTUREBUS+ bus technology, Using the VHDL programming language. I wrote the VHDL program on the SUN platform (SUN OS version 3.0).</p><p>I documented the application in a detailed technical white paper entitled "Verification Methods for Hardware Construction". This paper was released to all programmers and hardware engineers at Ericsson and KTH. A copy is available for your review.</p><p>ABB Atom AB, Vasteras, Sweden	</p><p><strong>Team Leader, Architect, Hands on Developer Consultant.</strong></p><p>Electrical and Computer Lab--section SLC3:</p><p>Developed a series of utility programs / application in Quick BASIC (version 5.0) used for calibration of computer operated measurement equipment in the nuclear power plant reactors. Programs were run on the HP 9000/300, and Intel 286 platforms.</p><p><strong>TECHNOLOGY</strong></p><ul><li>Data Warehouse, Data Mart, OLAP, OLTP Databases, Teradata, Netezza, Oracle, Parallel Data Warehouse (PDW), SQL Server, MDM, MDS, Data Quality (DQ), Spark,<strong> </strong>Hadoop, Hortonworks, Cloudera, Apache KafKa, Hive, Impala, Flume, Sqoop, Map/Reduce, Pig, HDInsight, HBase, Storm, oozie, Python, Scala, HDFS, StreamInsight, PolyBase, Microsoft SSIS, SSAS, SSRS, ETL, BI, MDX, PL/SQL, TSQL, ERwin, Enterprise Architecture (EA),  SQL Servere 2000/2005/2008/2012/2014, Power Query, Power Map, PowerPivot, Power View, IBM Cognos, SPSS, InfoSphere DataStage, Informatica PowerCenter, SAP BusinessObjects (BO), SAP HANA, Crystal Reports, Hyperion, MicroStrategy, SharePoint 2007/2010/2013, Nintex, SharePoint Social, Collaboration, Record Management, Search, Web forms, InfoPath, Branding, CSOM, JSOM, PerformancePoint, Clustering, Failover, Web Analytics, Google Visualization, .NET 1.1 to 4.5, C#, WCF, Restful Services, WPF/Silverlight, WF, VB .NET, ASP .NET, ADO.NET, LINQ, MVC, MVVM, MVP, AJAX, Visual Studio, Dashboard Designer, SharePoint Designer, Visio, TFS, Cloud, Azure, PaaS, SaaS, IaaS, HTML 5.0, DHTML, XML, XSL, WSDL, XSD, JSON, COM, DCOM, MFC, C, Visual C++, Visual Basic, PowerShell scripts, and SDKs, DocXpress, BI Documentation, Nintex, SharePoint Social, Collaboration, Record Management, Search, Web forms, Branding.</li></ul><p>SKILLS</p><ul><li><strong>Big Data,</strong> Hadoop, Spark, Cloudera, Hortonworks, Storm, KafKa, Hive, Impala, Flume, Sqoop, MapReduce, Pig, HDInsight, HBase, oozie, Tableau Power BI and Cloudera visualization</li><li><strong>Cloud </strong>Azure, AWS, , HDInsight, Cortana Intelligence Suite, Data Factory, Data Gateway, Infrastructure as a Service IaaS, Platform as a Service PaaS, Microsoft R Server, NLB, Key phrase extraction Azure search, Unstructured text analytics, Event hub, Streaming, Poly Base</li><li><strong>Search engines </strong>Elasticsearch, SOLR, Lucene, Kibana. Logstash, Rsync, Tika</li><li><strong>Machine Learning and recommendation engines </strong>MLlib, TensorFlow, Keras, Weka Mahout, Multilayer perceptron classifier (MLPC), the feedforward artificial neural network, scikit-learn, Pandas, Deeplearning4j, Sparkling Water ML, Caffe2, MxNet etc. Different algorithms K-Means, Random Forest, Gradient Boosting algorithms (GBM, XGBoost, XGBoost and CatBoost)</li><li><strong>BI Framework: </strong>Strategy and Implementation Plans, Enterprise Metrics, Integration Points, Gap Analysis, BI Portfolio, Performance Management (PM), Analytic and PM Technologies, Defining Business and Decision Process, Building Metadata and Services Centers, Establishing Enterprise Information Management (EIM) Committees, Defining The Role of DW and BI Program Steering Committee, It's Mission, Objectives, Roles and Responsibilities, DAMA DMBOK</li><li><strong>Architecture and Data Modeling:</strong> Initial Conceptual Solution, Solution Blueprints, Technology Impact Analysis (TIA), Gap Analysis, Technology Roadmap, Dimensional modelling, ER Modelling, Start Schema, Snowflake, Fact, Dimension, Hierarchy, Inmon/ Kimball/ Imhoff, Data Marts, EDW, ERWin 9.5/8.0/7.x, DeZign, Microsoft Visio, Enterprise Architecture (EA), Service Oriented Architecture (SOA), UML, Zachman, TOGAF, Star &amp; Snowflake Schemas, 3 Normal Forms, Normalization and Demoralization, Logical Model and Physical Model, Fact/Dimension/Hierarchy identifications, Data Warehouse Development Lifecycle, Data Mapping, Data Dictionaries</li><li><strong>Data Governance:</strong> IBM InfoShere MDM, Informatica MDM, MDS, DQS, Profisee Maestro, SAS MDM </li><li><strong>Integration and ETL: </strong>SSIS/SSRS/SSAS, SQL Server 2014/2012/2008R2/2008/2005, Informatica PowerCenter, DataStage, Cognos, ETL Mapping design, Data Profiling, Data Validation, Data Migration, Data Cleansing, Data Structure, Data Quality Services (DQS), BIDS, SQL Data Tools (SSDT), Auditing Framework, Execution Plans, ETL Parallel Processing, Error Handling, Custom Scripting, IBM Cognos, InfoSphere DataStage, Informatica PowerCenter, SAP BusinessObjects (BO)</li><li><strong>Data Warehousing and Analysis: </strong>OLAP/Cube/MDX/DAX, Dimensional Modelling, Tabular Modelling, KPIs, KPPIs, Data Analysis, SPSS, Predictive Analysis, Data Mining, Machine Learning, SAP HANA, Statistical Analysis, SAS, SAS VA (Visual Analytics), R, XLSTAT, Sentiment analysis, Speech analytics, Teradata. Netezza, Cloudera, PDW, Aginity, Master Data Services (MDS), Master Data Management (MDM), Data Quality (DQ), Analysis of Change (AOC), Metric Engine. </li><li><strong>Reporting: </strong>Predefined Reports, Ad-hoc Reporting, Analytical Reports, Custom Reporting with .NET/ Report Viewer, SQL Server Reporting Services (SSRS), SharePoint 2013/2010/2007/2003, PerformancePoint, PowerPivot, Power View, Crystal Reports, Hyperion, MicroStrategy, Cognos Report Studio, Framework, Workspace Advanced, DMR, TM1</li><li><strong>Data Visualization: </strong>Power Map, PowerPivot, Power View, SharePoint, Liferay, PerformancePoint, Google Visualization, Esri's GIS (geographic information systems) , mapping, SAP Lumira, QlikView, Tableau, Data Mapping </li><li><strong>Database: </strong>MS-SQL, Oracle, Oracle SQL Developer, TSQL, MDX, DMX, PL/SQL, Stored Procedure, View, Function, Erwin Data Modeler, DB2, PowerDesigner, MongoDB, Access, Excel, FoxPro, Informix, NoSQL, Big-data, Hadoop, Spark, HBase, HDInsight, PDW, PolyBase, Hive, HQL, Map/Reduce, HFS, Alert</li><li><strong>Programing Languages: </strong>SQL, T-SQL, PL/SQL, C#, WCF, Restful Services, WPF/Silverlight, WF, VB .NET, ASP .NET, ADO.NET, LINQ, MVC, MVVM, MVP, AJAX, HTML 5.0, DHTML, XML, XSL, WSDL, XSD, JSON, Java Script, PowerShell, COM, DCOM, VB Script, UNIX Shell Scripting</li><li><strong>Others: </strong>Agile, Extreme Programing, RUP, Use Cases, SDLC, TCP/IP, CVS, Microsoft Team Foundation Server (TFS), Tortoise SVN, SQL*Plus, TOAD, WinSQL, SilverLight, LightSwitch, Kerberos, Single Sign-On, Datazen, One-Key.</li><li><strong>Architecture and Design: </strong>Enterprise Architecture (EA), Service Oriented Architecture (SOA), Enterprise Service Bus (ESB), Top-Down/ Bottom-Up Design, Structured Design, Object Oriented Design, Multi-tiered and Multi-threaded architecture, Rational Rose, UModel, Patterns: Model/View/ViewModel (MVVM), MVC, MVP, Visio, UML, Zachman, TOGAF, Federal Enterprise Architecture, Gartner Methodology</li><li><strong>Business Optimization: </strong>Asset management, Information Technology Infrastructure Library (ITIL), customer satisfaction, call center management, service request enhancement, AODA compliance, fraud detection, CRM and ERP optimization, improving marketing effectiveness, portfolio optimization, governance, risk management, compliance, healthcare patient records management, electronic medical records (EMR), optimizing routes and schedules for logistics planning, insurance risk assessment, optimizing manufacturing production.</li><li><strong>Integration: </strong>Windows API, Biztalk, SOA, WCF, SSIS</li><li>Data Access: ADO.NET, LINQ, Entity Framework, Microsoft Enterprise Library, OLE DB, Oracle Data Provider, MS OLAP, SQL Master Data Services (MDS), StreamInsight</li><li><strong>Software Development</strong></li><li><a id="OLE_LINK16"></a><a id="OLE_LINK15"></a>Methodologies: Test driven programming, Agile software development, Extreme Programming (XP) Microsoft .NET Framework (from 1.0 to 4.0), C#, Visual Basic .NET, <a id="OLE_LINK18"></a><a id="OLE_LINK17"></a>VB .NET, ADO .NET, WinFX including Windows communication foundation (WCF), windows workflow (WF), windows presentation foundation (WPF), XAML, XML, HTML, HTML5, Java J2EE, Spring Framework, JavaScript, AJAX, RESTful services, Payment Card Industry (PCI), Image Processing</li><li>Visual C++ (MFC, SDKs, COM, DCOM, ATL ActiveXs), VB, C++, Perl, VHDL, Verilog, Shell, Skill, Ocean, SystemC</li><li>Scala SBT, Java Maven</li><li>Version control tools: Source Safe, Team Foundation Version Control, (TFVC), Subversion Tortoise SVN </li><li>Code metrics: Simian, RSM</li><li>Type/ industry: financial, banking, biomedical, pharmaceutical, engineering, telecommunication, semiconductor, logistics, health, scientific, e-commerce, instrumental</li><li><strong>Internet Development:</strong> ASP .NET, MVC, Sliverlight, HTML, DHTML, Web services for marketing and financial applications, AJAX, ASP, JavaScript and VB Script, XML, Microsoft Internet Information Server (IIS), Microsoft E-Commerce, PHP, Webload</li><li><strong>Cloud Computing</strong></li><li>Windows Azure, Amazon AWS EC2</li><li><strong>SharePoint: </strong>SharePoint 2013/2010/2007/2003, Multi-machine SharePoint Farm Architecture, Setup, Configuration, Load Balancing, Clustering, Backup Plans, Web Part and module development, Collaboration, Social, Search, Web Content Management, Enterprise Content Management, App Management, PerformancePoint and PowerPivot, PowerView, Application Federation, Secure Store Application, Business Connectivity, Usage Reports, SharePoint Designer, Dashboard Designer, PerformancePoint, dashboard, charts, KPI, Scorecards, reports, filters, Excel Services, PowerPivot Services, Web Analytics, Static Analysis, Hit Counters, Custom Development, PowerShell, SharePoint API, Object Model, web parts web services, workflows, Content Management, site collections/structure</li><li>Mobile Development</li><li>Android, iOS, Windows</li><li>Operating System Used: Windows, UNIX, Windows Azure, Linux, Android, iOS, Windows Mobile, MS-DOS</li><li>Hardware and Simulation: Matlab, Cadence Spectre, Spice, Eldo, ANSYS</li><li>Algorithms: Genetic algorithm, simulated annealing based algorithms, heuristic search, binary search, quick sort</li><li>Automation and Scripting: VB, Perl, Unix Shell</li></ul><p>LANGUAGES:</p><p>English, Swedish, Persian and Norwegian.</p>
    </div>
    <script>
        
        // Enable smooth scrolling
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });

        // Print functionality
        function printResume() {
            window.print();
        }

        // Download as PDF (requires backend support or browser print-to-PDF)
        function downloadAsPDF() {
            window.print();
        }
        
    </script>
</body>
</html>